{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "located-exception",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-applicant",
   "metadata": {},
   "source": [
    "<h1>REDES NEURONALES </h1>\n",
    "\n",
    "<strong>Una red neuronal toma valores de entrada y los procesa utilizando pesos \n",
    "    que son ajustados durante el proceso de entrenamiento. Esto le permite generar \n",
    "    una predicción a su salida. Los pesos son ajustados para encontrar patrones \n",
    "    que produzcan mejores predicciones. El diseñador de la red neuronal no necesita \n",
    "    especificar qué patrones deben buscarse, sino que la misma red los determina \n",
    "    durante el entrenamiento.</strong>\n",
    "\n",
    "Las redes neuronales están compuestas por <strong>capas</strong> y <strong>nodos</strong>. \n",
    "Las entradas de la red son las variables de entrada $x_1,...,x_n$ y el resultado de la \n",
    "última capa es el resultado de la función hipótesis $h_\\theta(x)$. \n",
    "<ul>\n",
    "    <li>La primera capa de una red neuronal se denomina <strong>capa de entrada</strong>, la cual \n",
    "        puede ser seguida por una o más <strong>capas ocultas</strong>. Y en último lugar se encuentra la\n",
    "        <strong>capa de salida</strong>.</li>\n",
    "    <li>Cada uno de los nodos (o neuronas) recibe una o más señales de entrada. Estas señales de entrada pueden\n",
    "        provenir de los datos de entrada o de alguna neurona posicionada en la capa anterior de la red neuronal.\n",
    "        Con estos datos, en cada nodo se realiza algún tipo de cálculo y se envía el resultado a neuronas \n",
    "        ubicadas en la siguiente capa de la red.</li>\n",
    "    <li>Cada unión de una capa con la siguiente posee un <strong>peso</strong> asociado.</li>\n",
    "    <strong>Los modelos de redes neuronales son entrenados principalmente mediante el ajuste de los pesos.</strong>\n",
    "</ul>\n",
    "\n",
    "<center>\n",
    "<img src=\"./figures/neural-network.png\"  height=\"600\" width=\"600\"/>\n",
    "</center>\n",
    "\n",
    "Cuando neurona, también llamada <strong>perceptron</strong> recibe sus \n",
    "señales de entrada procedentes de la capa anterior, suma estos valores \n",
    "multiplicados por su correspondiente peso. El valor obtenido luego es \n",
    "utilizado en una <strong>función de activación</strong>, la cual calcula \n",
    "el valor de salida de la neurona que será pasado a la siguiente capa.\n",
    "\n",
    "<center>\n",
    "<img src=\"./figures/node_neuralNetwork.png\"  height=\"600\" width=\"600\"/>\n",
    "</center>\n",
    "\n",
    "Es importante tener en cuenta que la primera variable recibida por cada \n",
    "neurona es siempre igual a 1 y es denominada <strong>bias unit</strong>.\n",
    "\n",
    "Existen muchas funciones de activación, una de ellas es la <strong>función \n",
    "sigmoide</strong>:\n",
    "          $$\n",
    "              g(z)=\\frac{1}{1+e^{-z}}\n",
    "          $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-vinyl",
   "metadata": {},
   "source": [
    "\n",
    "<h2>Red neuronal con una capa oculta</h2>\n",
    "\n",
    "Las redes neuronales que poseen al menos una capa oculta suelen denominarse\n",
    "<strong>Multi-Layer Perceptron</strong>.\n",
    "\n",
    "$$\n",
    "    \\begin{bmatrix}\n",
    "            x_0\\\\\n",
    "            x_1\\\\\n",
    "            x_2\\\\\n",
    "            x_3\n",
    "     \\end{bmatrix}\n",
    "     \\to\n",
    "     \\begin{bmatrix}\n",
    "            a_0^2\\\\\n",
    "            a_1^2\\\\\n",
    "            a_2^2\\\\\n",
    "            a_3^2\n",
    "     \\end{bmatrix}\n",
    "     \\to\n",
    "     h_\\theta(x)\n",
    "$$\n",
    "\n",
    "<ul>\n",
    "    <li>$a_i^j$ es el resultado de la función de activación del nodo $i$ de la capa $j$. Se llaman también <strong>nodos de activación</strong>.</li>\n",
    "    <li>$\\Theta^j$ es la matriz de pesos que mapean de la capa $j$ a la capa $j+1$.</li>\n",
    "    <li>$\\Theta_{i,k}^j$ es el peso que mapea desde la neurona $i$ de la capa $j$, \n",
    "    hasta el nodo $k$ de la capa $j+1$.</li>\n",
    "</ul>\n",
    "\n",
    "<center>\n",
    "<img src=\"./figures/nn_hiddenLayers.jpg\"  height=\"600\" width=\"600\"/>\n",
    "</center>\n",
    "\n",
    "Teniendo en cuenta la imagen anterior, el vector de variables\n",
    "de entrada con el nodo bias adicionado es de dimensión $1x4$.\n",
    "Es posible calcular el valor de entrada ($z$) a cada función \n",
    "de activación ($g(z)$) de los nodos de la capa oculta \n",
    "utilizando una matriz $\\Theta$ de tamaño $4x3$. El número de \n",
    "filas de esta matriz se corresponde con el número de \n",
    "variables de entrada ($x_1,x_2,x_3$) sumado al bias unit ($x_0$), \n",
    "y el número de columnas hace referencia al número de nodos de la \n",
    "capa oculta (sin incluir el bias unit).\n",
    "\n",
    "$$\n",
    "    a_1^2 = g\\left(x_0\\Theta_{01}^1+x_1\\Theta_{11}^1+x_2\\Theta_{21}^1+x_3\\Theta_{31}^1\\right) \n",
    "$$\n",
    "$$\n",
    "    a_2^2 = g\\left(x_0\\Theta_{02}^1+x_1\\Theta_{12}^1+x_2\\Theta_{22}^1+x_3\\Theta_{32}^1\\right) \n",
    "$$\n",
    "$$\n",
    "    a_3^2 = g\\left(x_0\\Theta_{03}^1+x_1\\Theta_{13}^1+x_2\\Theta_{23}^1+x_3\\Theta_{33}^1\\right) \n",
    "$$\n",
    "$$\n",
    "    a^2 = g\\left(X.\\Theta^1\\right)\n",
    "$$\n",
    "\n",
    "En cada capa, el nodo de activación correspondiente al valor de $\\Theta_0^j$, \n",
    "es el denominado bias node y posee un valor de 1.\n",
    "$$\n",
    "    a_0^2 = 1\n",
    "$$\n",
    "\n",
    "El resultado de la hipótesis es el resultado de la aplicación de la función de activación a la suma de los \n",
    "valores de los nodos de activación de la segunda capa, los cuales son multiplicados por una segunda matriz \n",
    "$\\Theta$ de dimensión $4x1$, que contiene los pesos correspondientes para los nodos de dicha capa de la red.\n",
    "\n",
    "$$\n",
    "    a_1^3 = g\\left(a_0^2\\Theta_{01}^2+a_1^2\\Theta_{11}^2+a_2^2\\Theta_{21}^2+a_3^2\\Theta_{31}^2\\right) = g\\left(a^2.\\Theta^2\\right) =h_\\Theta(x)\n",
    "$$\n",
    "\n",
    "<strong>Si una red posee $s_j$ nodos en la capa $j$ y $s_{j+1}$ nodos en la capa $j+1$, la matriz $\\Theta^j$\n",
    "será de dimensión $(s_j+1) . s_{j+1}$, siendo este 1 adicionado debido al bias node.</strong>\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mobile-paint",
   "metadata": {},
   "source": [
    "<h2>CLASIFICACIÓN MULTICLASE</h2>\n",
    "Para realizar una clasificación en múltiples clases, la función hipótesis debe dar como resultado un <strong>vector de valores</strong>. \n",
    "\n",
    "$$\n",
    "            \\begin{bmatrix}\n",
    "                    x_0\\\\\n",
    "                    x_1\\\\\n",
    "                    ...\\\\\n",
    "                    x_2\n",
    "             \\end{bmatrix}\n",
    "             \\to\n",
    "             \\begin{bmatrix}\n",
    "                    a_0^2\\\\\n",
    "                    a_1^2\\\\\n",
    "                    ...\n",
    "             \\end{bmatrix}\n",
    "             \\to\n",
    "             \\begin{bmatrix}\n",
    "                    a_0^3\\\\\n",
    "                    a_1^3\\\\\n",
    "                    ...\n",
    "             \\end{bmatrix}\n",
    "             \\to\n",
    "             ...\n",
    "             \\to\n",
    "             \\begin{bmatrix}\n",
    "                    h_\\theta(x)_1\\\\\n",
    "                    h_\\theta(x)_2\\\\\n",
    "                    ...\n",
    "             \\end{bmatrix}      \n",
    "$$\n",
    "\n",
    "\n",
    "Por ejemplo, si se desea clasificar los datos en 4 categorías, se puede definir un set de clases resultantes, de las cuales cada \n",
    "$y^i$ representa una clase diferente:\n",
    "\n",
    "$$\n",
    "    y^i \n",
    "    =\n",
    "    \\begin{bmatrix}\n",
    "       1\\\\\n",
    "       0\\\\\n",
    "       0\\\\\n",
    "       0\n",
    "    \\end{bmatrix}\n",
    "    ,\n",
    "    \\begin{bmatrix}\n",
    "       0\\\\\n",
    "       1\\\\\n",
    "       0\\\\\n",
    "       0\n",
    "    \\end{bmatrix}\n",
    "    , \n",
    "    \\begin{bmatrix}\n",
    "       0\\\\\n",
    "       0\\\\\n",
    "       1\\\\\n",
    "       0\n",
    "    \\end{bmatrix}\n",
    "    ,\n",
    "    \\begin{bmatrix}\n",
    "       0\\\\\n",
    "       0\\\\\n",
    "       0\\\\\n",
    "       1\n",
    "    \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "christian-builder",
   "metadata": {},
   "source": [
    "<h2>FUNCIÓN DE COSTOS</h2>\n",
    "\n",
    "La función de costos para redes neuronales es:\n",
    "\n",
    "$$\n",
    "    J(\\Theta)= \\frac{-1}{m}\\sum_{i=1}^m\\sum_{k=1}^K\\left[y_k^i\\log\\left((h_\\theta(x^i))_k\\right)+(1-y_k^i)\\log\\left(1-(h_\\theta(x^i))_k\\right)\\right] \n",
    "$$\n",
    "\n",
    "<ul>\n",
    "    <li> $L$: número total de capas en la red.</li>\n",
    "    <li> $s_l$: número de nodos en la capa l (sin contar el bias node).</li>\n",
    "    <li> $K$: número de nodos/clases de salida.</li>\n",
    "    <li> $h_\\theta(x)_k$: hipótesis resultante de la salida k.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-adapter",
   "metadata": {},
   "source": [
    "<h2>ALGORITMO DE PROPAGACIÓN HACIA ATRÁS</h2>\n",
    "\n",
    "El algoritmo de propagación hacia atrás (backpropagation) es un algoritmo de minimización de la función costo $J(\\Theta)$.\n",
    "El mismo busca obtener el set óptimo de parámetros $\\Theta$ que den por resultado $\\min_{\\Theta} J(\\Theta)$, para lo cual calcula las derivadas parciales $\\frac{\\partial}{\\partial\\Theta_{i,j}^l}J(\\Theta)$.\n",
    "\n",
    "Dado un set de datos de entrenamiento: $\\{(x^1,y^1),...,(x^m,y^m)\\}$:\n",
    "\n",
    "<ol>\n",
    "    <li>Setear $\\Delta_{i,j}^l:=0$ para todo valor de $l,i,j$.</li>\n",
    "    <li>Para cada ejemplo de entrenamiento $t=1,...m$:\n",
    "        <ol>\n",
    "            <li>Setear $a^1:= x^t$</li>\n",
    "            <li>Realizar la propagación hacia adelante (forward) para calcular los valores de $a^l$:\n",
    "                $$\n",
    "                    a^1=X\n",
    "                $$\n",
    "                $$\n",
    "                    Z^2=\\Theta^1 a^1\n",
    "                $$\n",
    "                $$\n",
    "                    a^2 = g(Z^2)\n",
    "                $$\n",
    "                $$\n",
    "                    Z^3=\\Theta^2 a^2\\quad \\text{con $a_0^2$ adicionado}\n",
    "                $$\n",
    "                $$\n",
    "                    a^3=g(Z^3)\n",
    "                $$\n",
    "                $$\n",
    "                    Z^4=\\Theta^3 a^3\\quad \\text{con $a_0^3$ adicionado}\n",
    "                $$ \n",
    "                $$\n",
    "                    a^4 = h_\\Theta(x)= g(Z^4)\n",
    "                $$\n",
    "            </li>\n",
    "            <li>Usando el valor $y^i$, calcular $\\delta^L=a^L-y^t$.\n",
    "                <ul> \n",
    "                    <li>$a^L$ es el vector de salidas de los nodos de activación de la última capa de la red.\n",
    "                Por lo que los <strong>valores de error ($\\delta^L$)</strong> de la última capa son\n",
    "                la diferencia entre los resultados de la última capa y las salidas correctas en $y$.</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>Calcular los valores de $\\delta^{L-1},\\delta^{L-2},...,\\delta^2$.\n",
    "                <ul> \n",
    "                    <li>Los valores $\\delta^l$ son calculados para cada capa mediante la multiplicación de los\n",
    "                    valores $\\delta$ de la siguiente capa($\\delta^{l+1}$) con la matriz $\\Theta^l$ de la capa actual. Seguido \n",
    "                    por la multiplicación elemento a elemento de este resultado con la derivada de la función\n",
    "                    de activación $g$ evaluada en $Z^l$:\n",
    "                    $$\n",
    "                        g(Z^l)=a^l.*(1-a^l)\n",
    "                    $$\n",
    "                    $$\n",
    "                        \\delta^l=\\left((\\Theta^l)^T\\delta^{l+1}\\right).*a^l.*(1-a^l)\n",
    "                    $$\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>Una vez calculados los valores $\\delta$, se actualizan los valores de $\\Delta$:\n",
    "                $$\n",
    "                    \\Delta_{i,j}^l:=\\Delta_{i,j}^l+a_j^l\\delta_i^{l+1}\n",
    "                $$\n",
    "                Finalmente estos valores se acumulan en la matriz $D$, para luego obtener la derivada parcial, \n",
    "                ya que $\\frac{\\partial}{\\partial \\Theta_{i,j}^l}J(\\Theta)=D_{i,j}^l$: \n",
    "                $$\n",
    "                    D_{i,j}^l := \\frac{1}{m}\\left(\\Delta_{i,j}^l+ \\lambda\\Theta_{i,j}^l\\right)\\quad j\\neq 0\n",
    "                $$\n",
    "                $$\n",
    "                    D_{i,j}^l := \\frac{1}{m}\\Delta_{i,j}^l\\quad j=0\n",
    "                $$\n",
    "            </li>\n",
    "        </ol>\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressed-flash",
   "metadata": {},
   "source": [
    "<h2>FUNCIONES DE ACTIVACIÓN</h2>\n",
    "          \n",
    "<table width=\"750\">\n",
    "  <tr>\n",
    "      <th style=\"text-align:center\">MÉTODO</th>\n",
    "      <th style=\"text-align:center\">USO</th>\n",
    "      <th style=\"text-align:center\">DESVENTAJA</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "      <th style=\"text-align:center\">Sigmoid</th>\n",
    "      <td style=\"text-align:center\">Cuando se requieren valores de salida en el intervalo $(0,1)$.</td>\n",
    "      <td style=\"text-align:center\">Sufre de desvanecimiento de gradiente.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "      <th style=\"text-align:center\">Hyperbolic Tangent</th>\n",
    "      <td style=\"text-align:center\">Cuando se requieren valores de salida en el intervalo $(-1,1)$.</td>\n",
    "      <td style=\"text-align:center\">Sufre de desvanecimiento de gradiente.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th style=\"text-align:center\">ReLU</th>\n",
    "      <td style=\"text-align:center\">Para capturar efectos grandes, no sufre de desvanecimiento de gradiente.</td>\n",
    "      <td style=\"text-align:center\">.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th style=\"text-align:center\">Leaky ReLu</th>\n",
    "      <td style=\"text-align:center\">Actúa como un ReLU pero permitiendo valores de salida negativos .</td>\n",
    "      <td style=\"text-align:center\">.</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "El problema de desvanecimiento del gradiente se debe al hecho de que a \n",
    "medida que la red tiene más capas, el gradiente se vuelve cada vez más pequeño \n",
    "en las primeras capas. Por esta razón, otras funciones se han vuelto más comunes.\n",
    "\n",
    "\n",
    "La función de activación correcta depende de la aplicación y no existen reglas\n",
    "estrictas y rápidas para su selección. Estas son algunas de las funciones de \n",
    "activación más utilizadas:\n",
    "<center>\n",
    "<img src=\"./figures/activationFunctions.png\"  height=\"600\" width=\"600\"/>\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
