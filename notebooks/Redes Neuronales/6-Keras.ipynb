{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ab4979e-4e08-4de0-b76d-89df71a69cfa",
   "metadata": {},
   "source": [
    "<h1>REDES NEURONALES CON KERAS</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cb654d-16ab-446a-b72f-9a30cd22e55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------\n",
    "# Project    : Erasmus + NEON\n",
    "# Author     : Yasmin Mondino Llermanos\n",
    "# Affiliation: National University of Cordoba (UNC) \n",
    "# E-mail     : yasmin.mondino@mi.unc.edu.ar \n",
    "#---------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stopped-interstate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-detection",
   "metadata": {},
   "source": [
    "\n",
    "Para defenir redes neuronales en TensorFlow se hace uso de <strong>Keras</strong>.\n",
    "\n",
    "El valor de salida de una neurona puede expresarse como $y = \\sigma (Wx+b)$, donde\n",
    "$W$ representa la matriz de pesos, $b$ es el término bias, $x$ es la entrada y \n",
    "$\\sigma$ es la función de activación. \n",
    "\n",
    "<center>\n",
    "<img src=\"./figures/perceptrons.jpeg\"  height=\"500\" width=\"500\"/>\n",
    "</center>\n",
    "\n",
    "$$\n",
    "\\boxed{\\begin{array}{l}\n",
    "    z & = \\text{tf.matmul}(x, W) + b \\\\\n",
    "    y & = \\text{tf.sigmoid}(z)\n",
    "\\end{array}}\n",
    "$$\n",
    "\n",
    "<h2>CLASE LAYERS</h2>\n",
    "La clase <strong>Layers</strong> es un bloque utilizado para la construcción de redes neuronales. El mismo permite implementar operaciones de redes neuronales comunes y se utiliza para actualizar pesos, calcular pérdidas y definir la conectividad entre capas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hybrid-administrator",
   "metadata": {},
   "source": [
    "<div  class=\"alert alert-block alert-warning\">\n",
    "<b>Métodos de la clase Layer:</b> \n",
    "    <ol>\n",
    "        <li><b>__init__()</b>: Definir atributos de capa personalizados.</li>\n",
    "        <li><b>build(self, input_shape)</b>: Crear los pesos que dependen de la forma de los tensores de entrada.</li>\n",
    "        <li><b>call(self, inputs, *args, **kwargs)</b>: Realizar la lógica de aplicar la capa a los tensores de entrada (que deben pasarse como argumento)</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "important-bahamas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir una capa de una Red Neuronal\n",
    "\n",
    "class OurDenseLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, n_output_nodes):\n",
    "    super(OurDenseLayer, self).__init__()\n",
    "    self.n_output_nodes = n_output_nodes\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    d = int(input_shape[-1])\n",
    "    \n",
    "    self.W = self.add_weight(\"weight\", shape = [d, self.n_output_nodes]) \n",
    "    self.b = self.add_weight(\"bias\", shape = [1, self.n_output_nodes]) \n",
    "    \n",
    "  def call(self, x):\n",
    "    z = tf.matmul(x, self.W) + self.b \n",
    "\n",
    "    y = tf.sigmoid(z)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "layer = OurDenseLayer(3)\n",
    "layer.build((1,2))\n",
    "\n",
    "x_input = tf.constant([[1,2.]], shape = (1,2))\n",
    "\n",
    "y = layer.call(x_input)\n",
    "\n",
    "print(y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swedish-execution",
   "metadata": {},
   "source": [
    "<div  class=\"alert alert-block alert-info\">\n",
    "<b>Atributos de la clase Layer:</b> \n",
    "    <ol>\n",
    "        <li><b>trainable_weigths</b></li>\n",
    "        <li><b>non_trainable_weigths</b></li>\n",
    "        <li><b>weigths</b></li>\n",
    "    </ol>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "periodic-infection",
   "metadata": {},
   "source": [
    "<h2>DENSE LAYER</h2>\n",
    "TensorFlow tiene definidas varios tipos de capas (Layers) que son comunmente utilizadas en redes \n",
    "neuronales, como por ejemplo, las capas densas o completamente conectadas \n",
    "(<strong>Dense</strong>). Dense implementa la operación: <strong>output = activation(dot(input, kernel) + bias)</strong>.\n",
    "Donde kernel es una matriz de pesos creada por la capa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fixed-dayton",
   "metadata": {},
   "source": [
    "<div  class=\"alert alert-block alert-success\">\n",
    "    <b>Definir una capa densa</b> (existen más argumentos):\n",
    "    <dd>\n",
    "        <b>tf.keras.layers.Dense</b> (units, activation = None, use_bias = True,\n",
    "        kernel_initializer = 'glorot_uniform',\n",
    "        bias_initializer = 'zeros')\n",
    "    </dd>\n",
    "    \n",
    "</div>\n",
    "\n",
    "<ul>\n",
    "    <li><b>units</b>: Número entero positivo que determina la \n",
    "            dimensionalidad del espacio de salida.</li>\n",
    "    <li><b>activation</b>: Función de activación a utilizar. En caso de no \n",
    "            especificarse no se aplica ninguna (activación \"linear\").</li>\n",
    "    <li><b>use_bias</b>: Valor booleano que determina el uso o no del vector bias.</li>\n",
    "    <li><b>kernel_initializer</b>: Define la manera en que se setea la matriz de pesos inicialmente. 'glorot_uniform' selecciona muestras de una distribución uniforme entre [-limit, limit], donde limit = sqrt(6 / (fan_in + fan_out)) (fan_in es el número de unidades de entrada al tensor de pesos y fan_out es el número de unidades de salida).</li>\n",
    "    <li><b>bias_initializer</b>: Define la manera en que se setea el vector de bias inicialmente. 'zeros' genera tensores inicializados en 0.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "textile-communication",
   "metadata": {},
   "source": [
    "<div  class=\"alert alert-block alert-danger\">\n",
    "    Las capas necesitan saber la forma de las entradas para poder crear los pesos. \n",
    "    Si no se especifica esta forma, la capa no tendrá pesos asignados. Los mismos\n",
    "    serán creados cuando la capa sea llamada por primera vez con una determinada entrada.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typical-cutting",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "dense_layer = Dense(2, activation ='sigmoid') \n",
    "print(f'La matriz de pesos de la capa es: {dense_layer.weights}.')\n",
    "\n",
    "out = dense_layer(tf.ones((1, 4))) \n",
    "print(f'La matriz de pesos de la capa luego de ser llamada es: {dense_layer.weights}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supposed-contamination",
   "metadata": {},
   "source": [
    "<h2>SEQUENTIAL MODEL</h2>\n",
    "Es posible utilizar <b>Sequential model</b> de Keras para crear redes neuronales \n",
    "apilando capas (Layers) como si fueran bloques. \n",
    "\n",
    "<center>\n",
    "<img src=\"./figures/sequential.jpeg\"  height=\"500\" width=\"500\"/>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fancy-carol",
   "metadata": {},
   "source": [
    "<div  class=\"alert alert-block alert-success\">\n",
    "<b>Definición del modelo Sequential:</b> \n",
    "    <dd>\n",
    "       model = keras.Sequential (layers = None) \n",
    "    </dd>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-gossip",
   "metadata": {},
   "source": [
    "<div  class=\"alert alert-block alert-warning\">\n",
    "<b>Métodos de la clase Sequential:</b> \n",
    "    <ol>\n",
    "        <li><b>add()</b>: Añade una instancia de Layer encima de la pila de capas.</li>\n",
    "        <li><b>pop()</b>: Remover la última capa del modelo.</li>\n",
    "        <li><b>predict()</b>: Genera predicciones de salida para las muestras de entrada.</li>\n",
    "        <li><b>summary()</b>: Imprime un resumen con información de la red. El número de \n",
    "            la columnda parámetros es la suma de los pesos y bias correspondientes a \n",
    "            cada determinada capa.</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-married",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir una Red Neuronal utilizando Sequential API\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "n_output_nodes = 3\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "dense_layer = Dense(n_output_nodes, activation ='sigmoid') \n",
    "\n",
    "model.add(dense_layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hindu-infrastructure",
   "metadata": {},
   "source": [
    "<div  class=\"alert alert-block alert-danger\">\n",
    "    Utilizar model.weights o model.summary() genera un error ya que el modelo no es creado hasta \n",
    "    que no recibe datos de entrada por primera vez. \n",
    "    <ul>\n",
    "        <li>El argumento <strong>input_shape</strong> en la primera capa del \n",
    "            modelo permite definir la forma de la entrada.</li>\n",
    "        <li>El objeto <strong>Input</strong> colocado antes de la primera \n",
    "            capa del modelo permite definir la forma de la entrada al mismo:\n",
    "            <dd>\n",
    "                <i>model.add (keras.Input (shape(2,)))</i>\n",
    "            </dd></li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-personality",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.pop()\n",
    "\n",
    "dense_layer = Dense(n_output_nodes, activation ='sigmoid', input_shape = (2,)) \n",
    "\n",
    "model.add(dense_layer)\n",
    "\n",
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scientific-bones",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = tf.constant([[1,2.]], shape = (1,2))\n",
    "\n",
    "model_output = model.predict(x_input)\n",
    "\n",
    "print(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-century",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unsigned-planner",
   "metadata": {},
   "source": [
    "<div  class=\"alert alert-block alert-info\">\n",
    "<b>Atributos del modelo Sequential:</b> \n",
    "    <ol>\n",
    "        <li><b>layers</b>: Obtener las distintas capas que componen el modelo.</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedicated-optics",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Número de capas del modelo: {len(model.layers)}.')\n",
    "print(f'Lista de capas del modelo: {model.layers}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-reminder",
   "metadata": {},
   "source": [
    "<div  class=\"alert alert-block alert-warning\">\n",
    "<b>Otros métodos de la clase Sequential:</b> \n",
    "    <ol>\n",
    "        <li><b>compile()</b>: Configurar el modelo para el entrenamiento.\n",
    "            <ul>\n",
    "                <li>optimizer: Controla la tasa de aprendizaje. 'adam' es un método \n",
    "                    de gradiente descendente estocástico que ajusta la tasa de \n",
    "                    aprendizaje durante el entrenamiento.</li>\n",
    "                <li>loss function: Especifica la función de costo utilizada \n",
    "                    para evaluar el modelo ('categorical_crossentropy', \n",
    "                    'mean_squared_error', entre otros).</li>\n",
    "            </ul>\n",
    "        <li><b>fit()</b>: Entrenar el modelo durante un número determinado de epochs \n",
    "            (iteraciones del dataset completo).</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-charles",
   "metadata": {},
   "source": [
    "<h2>CLASE MODEL</h2>\n",
    "Es posible definir redes neuronales mediante una <b>subclase de la clase Model</b>, la cual agrupa las capas para permitir el entrenamiento y la inferencia del modelo. La subclasificación ofrece la flexibilidad de definir capas, ciclos de entrenamiento, funciones de activación y modelos personalizados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "different-lemon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir una Red Neuronal utilizando Subclases\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "class SubclassModel(tf.keras.Model):\n",
    "\n",
    "  def __init__(self, n_output_nodes):\n",
    "    super(SubclassModel, self).__init__()\n",
    "    self.dense_layer = Dense(n_output_nodes, activation = 'sigmoid')\n",
    "    \n",
    "  def call(self, inputs):\n",
    "    return self.dense_layer(inputs)\n",
    "\n",
    "\n",
    "n_output_nodes = 3\n",
    "model = SubclassModel(n_output_nodes)\n",
    "\n",
    "x_input = tf.constant([[1,2.]], shape = (1,2))\n",
    "\n",
    "print(model.call(x_input).numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frequent-programming",
   "metadata": {},
   "source": [
    "Es posible utilizar argumentos booleanos en la función de llamada para especificar diferentes comportamientos de red, por ejemplo, diferentes comportamientos durante el entrenamiento y la inferencia. Supongamos que en algunos casos queremos que nuestra red simplemente genere la entrada, sin ninguna perturbación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-australia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir un modelo utilizando Subclases y especificando un comportamiento especial\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "class IdentityModel(tf.keras.Model):\n",
    "\n",
    "  def __init__(self, n_output_nodes):\n",
    "    super(IdentityModel, self).__init__()\n",
    "    self.dense_layer = tf.keras.layers.Dense(n_output_nodes, activation='sigmoid')\n",
    "\n",
    "  def call(self, inputs, isidentity = False):\n",
    "    x = self.dense_layer(inputs)\n",
    "    if isidentity: \n",
    "      return inputs \n",
    "    return x\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "damaged-intake",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_output_nodes = 3\n",
    "model = IdentityModel(n_output_nodes)\n",
    "\n",
    "x_input = tf.constant([[1,2.]], shape=(1,2))\n",
    "\n",
    "out_activate = model.call(x_input) \n",
    "out_identity = model.call(x_input, isidentity=True)\n",
    "\n",
    "print(f\"\"\"La salida con funcón de activación es: {out_activate.numpy()}.\n",
    "La salida con identidad es: {out_identity.numpy()}.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-leave",
   "metadata": {},
   "source": [
    "<h1>GRADIENTE DESCENDENTE CON TENSOR-FLOW</h1>\n",
    "<b>tf.GradientTape</b> se utiliza para registrar operaciones para calcular gradientes.\n",
    "\n",
    "Cuando se realiza un pase directo a través de la red, todas las operaciones se graban en una \"cinta\"; luego, para calcular el gradiente, la cinta se reproduce al revés. De forma predeterminada, la cinta se descarta después de reproducirla al revés. Esto significa que un tf.GradientTape solo puede calcular un gradiente, y las llamadas posteriores arrojan un error de tiempo de ejecución. Sin embargo, podemos calcular varios degradados sobre el mismo cálculo creando una cinta de degradado persistente.\n",
    "\n",
    "<div  class=\"alert alert-block alert-info\">\n",
    "<b>Argumentos de GradientTape:</b> \n",
    "    <ol>\n",
    "        <li><b>persistent</b>: Boolean controlling whether a persistent gradient tape is created. False by default, which means at most one call can be made to the gradient() method on this object..</li>\n",
    "        <li><b>watch_accessed_variables</b>: Boolean controlling whether the tape will automatically watch any (trainable) variables accessed while the tape is active. Defaults to True meaning gradients can be requested from any result computed in the tape derived from reading a trainable Variable. If False users must explicitly watch any Variables they want to request gradients from..</li>\n",
    "    </ol>\n",
    "</div>\n",
    "\n",
    "\n",
    "Primero, veremos cómo podemos calcular gradientes usando GradientTape y acceder a ellos para el cálculo. Definimos la función simple $ y = x ^ 2 $ y calculamos el gradiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "balanced-granny",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo del Gradiente con GradientTape \n",
    "\n",
    "x = tf.Variable(3.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  y = x * x\n",
    "\n",
    "dy_dx = tape.gradient(y, x)\n",
    "\n",
    "print(dy_dx.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-appearance",
   "metadata": {},
   "source": [
    "<div  class=\"alert alert-block alert-info\">\n",
    "<b>Métodos de GradientTape:</b> \n",
    "    <ol>\n",
    "        <li><b>gradient</b>: Computes the gradient using operations recorded in context of this tape..</li>\n",
    "    </ol>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limited-identification",
   "metadata": {},
   "source": [
    "Al entrenar redes neuronales, se utiliza la diferenciación y el gradiente \n",
    "descendente estocástico (SGD) para optimizar una función de pérdida. A \n",
    "continuación se presenta un ejemplo en el que se busca minimizar la función \n",
    "$ L = (x-x_f) ^ 2 $. Siendo $ x_f $ una variable para un valor deseado que se \n",
    "intenta optimizar y $ L $ una pérdida que se busca minimizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-ticket",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimización de una función con SGD\n",
    "\n",
    "x = tf.Variable([tf.random.normal([1])])\n",
    "print(\"X inicial: {}\".format(x.numpy()))\n",
    "\n",
    "learning_rate = 1e-2 \n",
    "history = []\n",
    "x_f = 4\n",
    "\n",
    "for i in range(500):\n",
    "  with tf.GradientTape() as tape:\n",
    "        loss = (x - x_f)**2 \n",
    "\n",
    "  grad = tape.gradient(loss, x) \n",
    "  new_x = x - learning_rate * grad \n",
    "  x.assign(new_x)\n",
    "  history.append(x.numpy()[0])\n",
    "\n",
    "\n",
    "plt.plot(history)\n",
    "plt.plot([0, 500],[x_f,x_f])\n",
    "plt.legend(('Predicted', 'True'))\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('x value');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba89391-8025-405e-b756-8a3910ca4219",
   "metadata": {},
   "source": [
    "<h1>CNN EN KERAS</h1>\n",
    "<dl> \n",
    "    <dt><strong>Conv2D</strong></dt>\n",
    "    <dd>\n",
    "        <div  class=\"alert alert-block alert-success\">\n",
    "         keras.layers.Conv2D(filters, kernel_size, strides = (1, 1), padding = 'valid', \n",
    "            data_format = None, dilation_rate = (1, 1), activation = None, use_bias = True, \n",
    "            kernel_initializer = 'glorot_uniform', bias_initializer = 'zeros', kernel_regularizer = None, \n",
    "            bias_regularizer = None, activity_regularizer = None, kernel_constraint = None, \n",
    "            bias_constraint = None, **kwargs)\n",
    "        </div>\n",
    "        <ul>\n",
    "            <li><strong>filters</strong>: El número de filtros utilizados en cada posición, \n",
    "            es decir, la profundidad de la salida.</li>\n",
    "            <li><strong>kernel_size</strong>: Una tupla que define el alto y ancho del kernel.</li>\n",
    "            <li><strong>strides</strong>: Una tupla que define el paso en cada dirección. Por\n",
    "            defecto posee un valor de (1,1).</li>\n",
    "            <li><strong>input_shape</strong>: Es requerido únicamente en la primera capa.</li>\n",
    "        </ul>          \n",
    "    </dd>\n",
    "    <br>\n",
    "    <dt><strong>MaxPooling2D<strong></dt>\n",
    "    <dd>\n",
    "        <div  class=\"alert alert-block alert-success\">\n",
    "         keras.layers.pooling.MaxPooling2D(pool_size = (2, 2), strides = None, padding = 'valid', data_format = None)\n",
    "        </div>\n",
    "        <ul>\n",
    "            <li><strong>pool_size</strong>: Una tupla que define el tamaño de la grilla\n",
    "                utilizada para realizar el pooling.</li>\n",
    "            <li><strong>strides</strong>: Por defecto se asume que el tamaño del paso es\n",
    "                igual al tamaño de la grilla. De no ser así debe ser especificado.</li>\n",
    "        </ul>  \n",
    "      </dd>\n",
    "    <br>\n",
    "    <dt><strong>Flatten</strong></dt>\n",
    "    <dd>\n",
    "        <div  class=\"alert alert-block alert-success\">\n",
    "        keras.layers.Flatten(data_format = None, **kwargs)\n",
    "        </div>\n",
    "        Transforma la entrada en un vector unidimensional. Por lo que es utilizado\n",
    "        generalmente para realizar el paso entre redes convolucionales a redes\n",
    "        completamente conectadas.\n",
    "    </dd>\n",
    "    <br>\n",
    "    <dt><strong>ZeroPadding2D</strong></dt>\n",
    "    <dd>\n",
    "        <div  class=\"alert alert-block alert-success\">\n",
    "        keras.layers.ZeroPadding2D(padding = (1, 1), data_format = None, **kwargs)\n",
    "        </div>\n",
    "        Añade filas o columnas de ceros arriba, abajo, a la izquierda y derecha del tensor de una imagen.\n",
    "    </dd>\n",
    "</dl>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e20c7d6-0458-4cf1-b5d8-d87fd60f74fc",
   "metadata": {},
   "source": [
    "[![CC BY 4.0][cc-by-shield]][cc-by]\n",
    "\n",
    "This work is licensed under a\n",
    "[Creative Commons Attribution 4.0 International License][cc-by].\n",
    "\n",
    "[![CC BY 4.0][cc-by-image]][cc-by]\n",
    "\n",
    "[cc-by]: http://creativecommons.org/licenses/by/4.0/\n",
    "[cc-by-image]: https://i.creativecommons.org/l/by/4.0/88x31.png\n",
    "[cc-by-shield]: https://img.shields.io/badge/License-CC%20BY%204.0-lightgrey.svg\n",
    "\n",
    "![logo_neon_erasmus](https://raw.githubusercontent.com/neon-iot/communication_labs/main/detection_theory/notebooks/images/BannerSupportErasmus_.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5230b3-cd57-47e2-aed0-0ebbab85dd7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
