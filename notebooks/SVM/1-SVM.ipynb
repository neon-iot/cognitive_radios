{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arranged-watts",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "%matplotlib widget\n",
    "layout = widgets.Layout(align_items = 'center')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-router",
   "metadata": {},
   "source": [
    "<h1>MÁQUINA DE VECTOR SOPORTE</h1>\n",
    "La máquina de vector soporte (Support Vector Machine, <b>SVM</b>) es un\n",
    "tipo de algoritmo supervisado de aprendizaje automático que es utilizado\n",
    "mayormente para problemas de clasificación, aunque también puede utilizarse\n",
    "para casos de regresión. Su mayor ventaja es que logra un muy buen \n",
    "desempeño con un menor costo computacional. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qualified-exemption",
   "metadata": {},
   "source": [
    "<h2>CLASIFICADOR DE VECTOR SOPORTE</h2>\n",
    "\n",
    "El clasificador de vector soporte es un hiperplano\n",
    "en un espacio N dimensional, siendo N el número de features o \n",
    "variables de entrada, que permite separar los datos en sus\n",
    "correspondientes clases. Es necesario tener en cuanta que, \n",
    "por ejemplo, para separar los datos en 2 clases existen muchos \n",
    "hiperplanos que podrían elegirse. Sin embargo, se busca el \n",
    "<strong>margen máximo</strong>, es decir, la máxima distancia entre el \n",
    "hiperplano y los datos de ambas clases \n",
    "más cercanos a él. \n",
    "\n",
    "<center>\n",
    "<img src=\"./figures/svm.png\"  height=\"600\" width=\"600\"/>\n",
    "</center>\n",
    "\n",
    "<dl> \n",
    "    <dt>HIPERPLANOS</dt>\n",
    "        <dd>\n",
    "            Son límites de decisión (umbrales) que ayudan a clasificar los datos según\n",
    "            se encuentren a un lado u otro de ellos. La dimensión de un \n",
    "            hiperplano depende del número de features.\n",
    "        </dd>\n",
    "    <br>\n",
    "    <dt>VECTORES SOPORTE</dt>\n",
    "        <dd>\n",
    "            Son los datos que se encuentran más cercanos al hiperplano e \n",
    "            influencian en la posición y orientación del mismo.\n",
    "        </dd>\n",
    "    <br>\n",
    "    <dt>MARGEN</dt>\n",
    "        <dd>\n",
    "            La menor distancia entre los datos (observaciones) y el hiperplano.\n",
    "            Cuando el umbral se encuentra en el punto medio entre las observaciones más\n",
    "            cercanas a él de cada clase, el margen es máximo y el clasificador se \n",
    "            denomina <strong>Clasificador de margen grande</strong> (Large Margin Classifier).\n",
    "        </dd>\n",
    "</dl>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative-welcome",
   "metadata": {},
   "source": [
    "<div  class=\"alert alert-block alert-info\"> \n",
    "   Mientras mayor sea el margen o distancia entre el hiperplano \n",
    "   y los datos de cada clase más cercanos a él de cada lado,\n",
    "   mayor es la confianza que se puede tener de que el clasificador \n",
    "    de vector soporte está tomando la decisión correcta al \n",
    "    clasificar los datos.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-bonus",
   "metadata": {},
   "source": [
    "<h2>HARD MARGIN vs SOFT MARGIN</h2>\n",
    "\n",
    "\n",
    "Los clasificadores de margen grande son extremadamente \n",
    "sensibles a los datos atípicos (outliers), es por ello que\n",
    "existen dos tipos de algoritmos:\n",
    "\n",
    "<dl> \n",
    "    <dt><strong>De margen duro</strong> (Hard Margin)</dt>\n",
    "        <dd>\n",
    "            Buscan encontrar un hiperplano sin ningún error\n",
    "            de clasificación (misclassification). Un modelo \n",
    "            generado con datos atípicos en el set de \n",
    "            entrenamiento puede sufrir de <strong>overfitting \n",
    "            ó high variance</strong>.\n",
    "        </dd>\n",
    "    <br>\n",
    "    <dt><strong>De margen blando</strong> (Soft Margin)</dt>\n",
    "        <dd>\n",
    "           Poseen una cierta tolerancia a errores de clasificación\n",
    "           (misclassifications). Esto genera que los datos atípicos \n",
    "           puedan ser clasificados erróneamente, pero el umbral \n",
    "           seleccionado será más correcto para utilizar con nuevos \n",
    "           datos. \n",
    "        <br>\n",
    "           Para determinar cuál es el margen blando más adecuado \n",
    "            para seleccionar, se hace uso de la <strong>validación \n",
    "            cruzada</strong>. De manera de determinar cuántos errores de \n",
    "            clasificación y observaciones pueden ser permitidos \n",
    "            dentro del margen blando para obtener la mejor clasificación.\n",
    "        </dd>\n",
    "</dl>\n",
    "\n",
    "<center>\n",
    "<img src=\"./figures/softAndHard.jpeg\"  height=\"800\" width=\"800\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wicked-router",
   "metadata": {},
   "source": [
    "<h2>FUNCIÓN COSTO</h2>\n",
    "En regresión logística, se toma la salida de una función lineal\n",
    "y se la transforma a un valor en el rango $[0,1]$ mediante la\n",
    "utilización de una función sigmoide $g(z)$.\n",
    "\n",
    "<ul>\n",
    "    <li>Si $y = 1$, entonces $h_\\Theta(x) = 1$ para $g(z)\\geq 0$, siendo $z = \\Theta^T x \\geq 0$.</li>\n",
    "    <li>Si $y = 0$, entonces $h_\\Theta(x) = 0$ para $g(z) < 0$, siendo $z = \\Theta^T x < 0$.</li>\n",
    "</ul>\n",
    "\n",
    "La función de costo correspondiente a la regresión logística\n",
    "sin regularización es:\n",
    "\n",
    "$$\n",
    "    J(\\theta)=-\\frac{1}{m}\\sum_{i_1}^m \\left[y^i\\log\\left(\\frac{1}{1+e^{-\\Theta^Tx^i}}\\right)\n",
    "                    +(1-y^i)\\log\\left(1-\\frac{1}{1+e^{-\\Theta^Tx^i}}\\right)\\right]\n",
    "             =\\frac{1}{m}\\sum_{i_1}^m cost(h_\\theta(x^i),y^i)\n",
    "$$\n",
    "\n",
    "$$\n",
    "    cost(h_\\theta(x^i),y^i)= -\\log(h_\\theta(x^i)) \\leftrightarrow y=1\n",
    "$$\n",
    "$$\n",
    "    cost(h_\\theta(x^i),y^i)= -\\log(1-h_\\theta(x^i)) \\leftrightarrow  y=0\n",
    "$$\n",
    "\n",
    "Para obtener la función costo de un SVM, simplemente se modifica el primer \n",
    "término de la función costo de regresión logística $-\\log(h_\\Theta(x))=\n",
    "-\\left(\\frac{1}{1+e^{-\\Theta^Tx}}\\right)$ \n",
    "de manera que cuando $\\Theta^T x > 1$ la salida sea 0 y para valores \n",
    "menores a 1, la función de decrecimiento sea lineal y no curva. De igual\n",
    "manera, se modifica el segundo término $-\\log(1-h_\\Theta(x))=-\\log\n",
    "\\left(1-\\frac{1}{1+e^{-\\Theta^Tx}}\\right)$ para que cuando $\\Theta^T x < -1$ \n",
    "la salida sea 0 y para valores mayores a -1, la función creciente sea lineal.\n",
    "\n",
    "<center>\n",
    "<img src=\"./figures/costFunctionSVM.png\"  height=\"600\" width=\"600\"/>\n",
    "</center>\n",
    "\n",
    "Por lo tanto, si $z=\\Theta^Tx$, estas funciones son:\n",
    "\n",
    "$$\n",
    "    cost_1(z)= max(0,(1-z)) \\leftrightarrow y=1\n",
    "$$\n",
    "$$\n",
    "    cost_0(z)= max(0,(1+z)) \\leftrightarrow  y=0\n",
    "$$\n",
    "\n",
    "Si reemplazamos estos términos en la función de costo para regresión logística\n",
    "con regularización:\n",
    "\n",
    "$$\n",
    "    J(\\theta)=\\frac{1}{m}\\sum_{i=1}^m\\left[y^i\\left(-\\log(h_\\theta(x^i))\\right)+(1-y^i)\\left(-\\log(1-h_\\theta(x^i))\\right)\\right]+\\frac{\\lambda}{2m}\\sum_{j=1}^n\\theta_j^2\n",
    "$$\n",
    "\n",
    "Se obtiene la función de costo para SVM:\n",
    "\n",
    "$$\n",
    "    J(\\theta)=\\frac{1}{m}\\sum_{i=1}^m\\left[y^icost_1(\\Theta^Tx)+(1-y^i)cost_0(\\Theta^Tx)\\right]+\\frac{\\lambda}{2m}\\sum_{j=1}^n\\theta_j^2\n",
    "$$\n",
    "\n",
    "Es común que se elimine el factor $m$ ya que el mismo no afecta a la \n",
    "optimización. A su vez, la regularización se realiza mediante \n",
    "el parámetro $C$ en lugar de $\\lambda$. Para aumentar la regularización \n",
    "el valor $C$ debe reducirse, mientras que para disminuir la regularización \n",
    "aplicada, el valor de $C$ debe ser mayor:\n",
    "\n",
    "$$\n",
    "    J(\\theta)= C \\sum_{i=1}^m\\left[y^icost_1(\\Theta^Tx)+(1-y^i)cost_0(\\Theta^Tx)\\right]+\\frac{1}{2}\\sum_{j=1}^n\\theta_j^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordered-plane",
   "metadata": {},
   "source": [
    "<div  class=\"alert alert-block alert-info\"> \n",
    "   La hipótesis de SVM no es interpretada como una probabilidad de \n",
    "   que $y$ sea 1 o 0, como se hace en el caso de la regresión logística.\n",
    "   Sino que la salida es simplemente 1 ó 0.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "centered-salad",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>UMBRAL DE DECISIÓN</h2>\n",
    "Para minimizar la función costo del SVM:\n",
    "<ul>\n",
    "    <li>Si $y=1$, no es suficiente que $\\Theta^Tx \\geq 0$, \n",
    "        es necesario que $\\Theta^Tx \\geq 1$.</li>\n",
    "    <li>Si $y=0$, no es suficiente que $\\Theta^Tx < 0$, \n",
    "        es necesario que $\\Theta^Tx \\leq -1$.</li>\n",
    "</ul>\n",
    "\n",
    "Si se considera un valor $C$ muy grande (una regularización muy\n",
    "pequeña), es necesario que los parámetros $\\Theta$ sean seleccionados\n",
    "de manera que el primer término de la función $J(\\Theta)$ sea cercano\n",
    "a cero para que la misma se minimice. De esta manera, la función costo \n",
    "que se desea minimizar queda definida como:\n",
    "\n",
    "$$\n",
    "    J(\\Theta) = C*0 + \\frac{1}{2}\\sum_{j=1}^n\\theta_j^2 \\quad \\text{siempre que }\n",
    "                \\left\\{\n",
    "                       \\begin{array}{ll}\n",
    "                     \\Theta^Tx \\geq 1  & \\mathrm{para\\ } y=1 \\\\\n",
    "                     \\Theta^Tx \\leq -1     & \\mathrm{para\\ } y=0\n",
    "                       \\end{array}\n",
    "                     \\right.\n",
    "$$\n",
    "\n",
    "El umbral de decisión para SVM tiene la propiedad de encontrarse \n",
    "<strong>lo más alejado posible</strong> tanto de\n",
    "los ejemplos positivos como de los negativos.\n",
    "La distancia del umbral de decisión con el ejemplo más cercano \n",
    "a el se denomina <b>margen</b>. Dado que SVM pretende maximizar \n",
    "el margen, se lo denomina también  clasificador de margen grande (Large \n",
    "Margin Classifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forty-twelve",
   "metadata": {},
   "source": [
    "<h2>FUNCIONES KERNEL</h2>\n",
    "SVM utiliza las denominadas funciones Kernel para encontrar sistemáticamente\n",
    "clasificadores de vector soporte de mayores dimensiones.\n",
    "\n",
    "<center>\n",
    "<img src=\"./figures/kernel.png\"  height=\"600\" width=\"600\"/>\n",
    "</center>\n",
    "\n",
    "<dl> \n",
    "    <dt>KERNEL LINEAL</dt>\n",
    "        <dd>$$\n",
    "                K(\\vec{x},\\vec{u})=\\vec{x} \\cdot \\vec{u}\n",
    "            $$</dd>\n",
    "    <br>\n",
    "    <dt>KERNEL POLINÓMICO</dt>\n",
    "        <dd>\n",
    "            Utiliza un parámetro $d$ que identifica el grado del polinomio e \n",
    "            incrementa las dimensiones sistemáticamente cambiando el valor de \n",
    "            este parámetro. Las relaciones entre cada par de observaciones\n",
    "            en estas dimensiones son utilizadas para encontrar un \n",
    "            clasificador de vector soporte.\n",
    "            La validación cruzada puede ser utilizada para encontrar el valor \n",
    "            óptimo de $d$\n",
    "            $$\n",
    "                K(\\vec{x},\\vec{u})=(\\gamma \\vec{x} \\cdot \\vec{u}+r)^d\n",
    "            $$\n",
    "        </dd>\n",
    "    <br>\n",
    "    <dt>KERNEL RADIAL (Gaussiano)</dt>\n",
    "        <dd>\n",
    "            Encuentra clasificadores de vector soporte en dimensiones\n",
    "            infinitas. En esta función, las observaciones más próximas a cada\n",
    "            nueva observación tienen una gran influencia en su clasificación.\n",
    "            Mientras que las observaciones que se encuentran alejadas tienen una\n",
    "            influencia muy pequeña.\n",
    "            $$\n",
    "                K(\\vec{x},\\vec{u})= e ^{- \\gamma \\left\\|\\vec{x} - \\vec{u}\\right\\|^2}\n",
    "            $$   \n",
    "        </dd>\n",
    "</dl>\n",
    "\n",
    "<h3>TRUCO DEL KERNEL</h3>\n",
    "Las funciones Kernel calculan únicamente las relaciones entre los pares de datos\n",
    "como si se encontraran en otra dimensión mayor, pero <strong>no realizan la \n",
    "transformación</strong> de los datos a estas dimensiones. Este hecho,\n",
    "denominado el <strong>truco del Kernel</strong> (Kernel Trick), permite\n",
    "reducir la cantidad de cálculos requeridos por las SVM, eliminando la \n",
    "matemática necesaria para transformar\n",
    "los datos de dimensiones pequeñas a mayores dimensiones."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
