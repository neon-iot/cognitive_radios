{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blocked-found",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "import warnings\n",
    "%matplotlib widget\n",
    "layout = widgets.Layout(align_items = 'center')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "better-helicopter",
   "metadata": {},
   "source": [
    "<h1>MEJORAS DEL MODELO</h1>\n",
    "<h2>EVALUACIÓN DE LA HIPÓTESIS</h2>\n",
    "Es posible que una función hipótesis tenga un error bajo para los datos de entrenamiento \n",
    "pero que igualmente sea incorrecta, por ejemplo en el caso de overfitting. Para \n",
    "evaluar una hipótesis, dado un set de datos de entrenamiento es posible dividir \n",
    "los datos en 2 sets:\n",
    "\n",
    "<ul>\n",
    "    <li><strong>SET DE ENTRENAMIENTO</strong> (training set): 70% de los datos</li>\n",
    "    <li><strong>SET DE PRUEBA</strong> (test set): 30% de los datos</li>\n",
    "</ul>\n",
    "\n",
    "A partir de estos 2 sets, se realiza un nuevo procedimiento:\n",
    "\n",
    "<ol>\n",
    "    <li>Se calculan los valores de $\\Theta$ que minimicen la función $J_{train}(\\Theta)$ utilizando el set de entrenamiento.</li>\n",
    "    <li>Se calcula el error del set de prueba $J_{test}(\\Theta)$</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powerful-pound",
   "metadata": {},
   "source": [
    "<div  class=\"alert alert-block alert-success\"> \n",
    "   Cuando el error de prueba, también llamado error de generalización \n",
    "    es pequeño, puede decirse que el modelo utilizado ha aprendido la \n",
    "    función que relaciona los datos de entrada y salida. Por lo que\n",
    "    es posible confiar en que el mismo funcione con otros datos nuevos.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-undergraduate",
   "metadata": {},
   "source": [
    "<dl> \n",
    "    <dt>ERROR DEL SET DE PRUEBA</dt>\n",
    "        <dd>\n",
    "            $$\n",
    "                J(\\Theta) = \\frac{1}{2m_{test}}\\sum_{i=1}^{m_{test}}\\left(h_{\\Theta}(x_{test}^i)-y_{test}^i\\right)^2\n",
    "            $$\n",
    "        </dd>\n",
    "</dl>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-graphics",
   "metadata": {},
   "source": [
    "<h2>SELECCIÓN DEL MODELO</h2>\n",
    "Que un algoritmo de aprendizaje se adapte bien a un conjunto de \n",
    "entrenamiento, no implica que sea una buena hipótesis. Podría \n",
    "darse el caso de overfitting por el que las predicciones para el\n",
    "set de prueba serían malas. <strong>El error de la hipótesis \n",
    "medido en el set de datos de entrenamiento será menor que el \n",
    "error en cualquier otro set de datos.</strong>\n",
    "\n",
    "Sin embargo, dados varios modelos, es posible \n",
    "testear cada uno de ellos y utilizar el valor del error resultante\n",
    "para seleccionar el modelo de hipótesis correcto.\n",
    "Para ello el set de datos inicial puede ser dividido en 3 nuevos sets:\n",
    "\n",
    "<ul>\n",
    "    <li><strong>SET DE ENTRENAMIENTO</strong> (training set): 60% de los datos originales, \n",
    "        utilizados para entrenar el modelo.</li>\n",
    "    <li><strong>SET DE VALIDACIÓN CRUZADA</strong> (cross validation set): 20% de los datos \n",
    "    originales, utilizados para seleccionar el modelo más apropiado y configurar \n",
    "    los denominados hiperparámetros: regularización, $\\alpha$ del gradiente descendente,\n",
    "    número de capas y nodos por capa de una red, etc.</li>\n",
    "    <li><strong>SET DE PRUEBA</strong> (test set): 20% de los datos originales, utilizados\n",
    "    para estimar el error de generalización.</li>\n",
    "</ul>\n",
    "\n",
    "A partir de estos sets, se calculan 3 errores diferentes:\n",
    "\n",
    "<ol>\n",
    "    <li>Se calculan los valores de $\\Theta$ que minimicen la función \n",
    "        $J_{train}(\\Theta)$ utilizando el set de entrenamiento para \n",
    "        cada uno de los distintos modelos.</li>\n",
    "    <li>Se selecciona el modelo que tenga el menor error \n",
    "        $J_{cv}(\\Theta)$ utilizando el set de validación cruzada.</li>\n",
    "    <li>Se estima el error de generalización $J_{test}(\\Theta)$ \n",
    "        utilizando el set de prueba.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precise-white",
   "metadata": {},
   "source": [
    "<div  class=\"alert alert-block alert-danger\"> \n",
    "   Si utilizamos el set de datos de validación cruzada para seleccionar \n",
    "   el modelo, no es posible utilizarlo también para\n",
    "   seleccionar el valor del parámetro $\\lambda$ de regularización.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charged-illustration",
   "metadata": {},
   "source": [
    "<h2>HIGH BIAS vs HIGH VARIANCE</h2>\n",
    "<div  class=\"alert alert-block alert-success\"> \n",
    "   <ol>\n",
    "       <li><strong>HIGH BIAS</strong> (UNDERFITTING): Tanto el error de \n",
    "           entrenamiento como el de validación cruzada son grandes: \n",
    "           $J_{cv}(\\Theta) \\approx J_{train}(\\Theta)$.</li>\n",
    "       <li><strong>HIGH VARIANCE</strong> (OVERFITTING): El error de \n",
    "           entrenamiento es pequeño, mientras que el de validación cruzada \n",
    "           es mucho mayor: $J_{train}(\\Theta) << J_{cv}(\\Theta)$.</li>\n",
    "   </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuing-techno",
   "metadata": {},
   "source": [
    "<h3>GRADO DEL POLINOMIO</h3>\n",
    "<ul>\n",
    "    <li>El ERROR DE ENTRENAMIENTO $J_{train}(\\Theta)$ tiende a <strong>decrecer\n",
    "        </strong> al aumentar el grado del polinomio.</li>\n",
    "    <li>El ERROR DE VALIDACIÓN CRUZADA $J_{cv}(\\Theta)$ tiende a decrecer\n",
    "        al aumentar el grado del polinomio hasta un cierto punto, a \n",
    "        partir del cual comienza a aumentar, formando una <strong>curva convexa\n",
    "        </strong>.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "racial-inside",
   "metadata": {},
   "source": [
    "<strong>EJEMPLO:</strong> Variación de los errores de entrenamiento y validación cruzada con el grado de polinomio del modelo:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-fraud",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    <li>A partir de un set de datos se obtiene un set de datos de entrenamiento y \n",
    "        uno de validación cruzada.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incomplete-religious",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('test.csv')\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-enzyme",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = data.loc[:,'x'].values.reshape(-1,1)\n",
    "y_data = data.loc[:,'y'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-circuit",
   "metadata": {},
   "source": [
    "Se utiliza la librería <strong>scikit-learn</strong>. Específicamente la función \n",
    "<strong>train_test_split</strong> permite dividir de manera aleatoria un set de \n",
    "datos en la proporción que se desee:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "persistent-confusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_cv, y_train, y_cv = train_test_split(x_data, y_data, test_size = 0.2,random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arbitrary-decade",
   "metadata": {},
   "source": [
    "<div  class=\"alert alert-block alert-warning\"> \n",
    "   <ul>\n",
    "       <li>Los resultados obtenidos varían según los datos que se seleccionen como \n",
    "           de entrenamiento.</li>\n",
    "       <li>El parámetro <strong>random_state</strong> controla el barajado aplicado \n",
    "        a los datos antes de ser divididos. Al pasar un entero, se obtienen los\n",
    "        mismos resultados en sucesivas llamadas a la función.</li>\n",
    "   </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "positive-journal",
   "metadata": {},
   "source": [
    "<ol start=\"2\">\n",
    "    <li>La clase <strong>LinearRegression</strong> permite crear un modelo de \n",
    "        regresión lineal, el cual es entrenado con el set de datos ($x_{train}^i,y_{train}^i$)</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "downtown-organ",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression(normalize = True)\n",
    "lin_reg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-allah",
   "metadata": {},
   "source": [
    "<ol start=\"3\">\n",
    "    <li>Se calcula el costo tanto para el set de entrenamiento como el de validación cruzada.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residential-johnston",
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunction(h, y):\n",
    "    m = h.shape[0]\n",
    "    J = 1/(2 * m) * np.sum(np.square(h - y))  \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "british-fighter",
   "metadata": {},
   "outputs": [],
   "source": [
    "Jtrain = np.array([costFunction(lin_reg.predict(x_train), y_train)])\n",
    "Jcv = np.array([costFunction(lin_reg.predict(x_cv), y_cv)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "primary-substitute",
   "metadata": {},
   "source": [
    "<ol start=\"4\">\n",
    "    <li>Para crear modelos de regresión polinómica, se debe definir en primer lugar \n",
    "        el grado del polinomio. Para ello se utiliza <strong>PolynomialFeatures</strong> \n",
    "        de la librería <strong>scikit-learn</strong>.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-signature",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "pf = PolynomialFeatures(degree = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifty-august",
   "metadata": {},
   "source": [
    "<ol start=\"5\">\n",
    "    <li>Se deben transformar las variables tanto del set de entrenamiento como el de validación cruzada para que tengan el grado deseado.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bearing-terrace",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_poly = pf.fit_transform(x_train)\n",
    "x_cv_poly = pf.fit_transform(x_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endangered-surveillance",
   "metadata": {},
   "source": [
    "<ol start=\"6\">\n",
    "    <li>Se utiliza nuevamente la clase <strong>LinearRegression</strong> para crear un modelo de \n",
    "        regresión lineal, el cual es entrenado con el set de datos ya transformado al grado correspondiente.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "straight-buffalo",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_reg = LinearRegression(normalize = True)\n",
    "poly_reg.fit(x_train_poly, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cubic-investment",
   "metadata": {},
   "source": [
    "<ol start=\"7\">\n",
    "    <li>Se calculan nuevamente el costo tanto para el set de entrenamiento como el de validación cruzada.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-cabin",
   "metadata": {},
   "outputs": [],
   "source": [
    "Jtrain = np.append(Jtrain,costFunction(poly_reg.predict(x_train_poly), y_train)) \n",
    "Jcv = np.append(Jcv,costFunction(poly_reg.predict(x_cv_poly), y_cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collectible-granny",
   "metadata": {},
   "source": [
    "<ol start=\"8\">\n",
    "    <li>Se puede utilizar una función que cree el modelo de regresión lineal del\n",
    "        grado deseado y lo entrene.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governing-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_poly(d, x_train):\n",
    "    pf = PolynomialFeatures(degree = d)\n",
    "    x_train_poly = pf.fit_transform(x_train)\n",
    "    \n",
    "    poly_reg = LinearRegression(normalize = True)\n",
    "    poly_reg.fit(x_train_poly, y_train)\n",
    "    \n",
    "    return pf, poly_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "toxic-genetics",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in range(3,25,1):\n",
    "    pf, poly_reg = fit_poly(d, x_train)\n",
    "    \n",
    "    x_train_poly = pf.fit_transform(x_train)\n",
    "    x_cv_poly = pf.fit_transform(x_cv)\n",
    "    \n",
    "    Jtrain = np.append(Jtrain,costFunction(poly_reg.predict(x_train_poly), y_train)) \n",
    "    Jcv = np.append(Jcv,costFunction(poly_reg.predict(x_cv_poly), y_cv))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prospective-furniture",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_d = widgets.Output()\n",
    "\n",
    "with plots_d:\n",
    "    figs_d, axs_d = plt.subplots(2,1,figsize = (8,8),tight_layout = True)\n",
    "    figs_d.suptitle(r'INFLUENCIA DEL GRADO DEL POLINOMIO')\n",
    "    \n",
    "    axs_d[0].set_title(fr'Hipótesis vs Valores de salida reales')\n",
    "    axs_d[0].scatter(x_data,y_data,marker= 'o',s = 30,color ='black', alpha = 0.3,label = 'Datos')\n",
    "    lineh, = axs_d[0].plot(x_data,lin_reg.predict(x_data),label ='Valores predichos')\n",
    "    axs_d[0].set_xlabel('x')\n",
    "    axs_d[0].set_ylabel('y')\n",
    "    axs_d[0].grid(True)\n",
    "    axs_d[0].legend()\n",
    "    \n",
    "    axs_d[1].set_title(fr'Variación de función costo $J(\\theta)$')\n",
    "    d = [i for i in range(1,len(Jtrain)+1)]\n",
    "    axs_d[1].plot(d, Jtrain, label = r'$J_{train}(\\Theta)$')\n",
    "    axs_d[1].plot(d, Jcv, label = r'$J_{cv}(\\Theta)$')\n",
    "    axs_d[1].set_xticks(d)\n",
    "    axs_d[1].set_xlabel('d')\n",
    "    axs_d[1].set_ylabel(r'$J(\\Theta)$')\n",
    "    axs_d[1].grid(True)\n",
    "    axs_d[1].legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fourth-affiliation",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_slider = widgets.IntSlider(min = 1, max = 24, value = 1, description = r'Grado')\n",
    "\n",
    "def update_plots_d (change):\n",
    "    if d_slider.value == 1:\n",
    "        x_predicted = lin_reg.predict(x_data)\n",
    "    else:\n",
    "        pf, poly_reg = fit_poly(d_slider.value, x_train)\n",
    "        x_data_poly = pf.fit_transform(x_data)\n",
    "        x_predicted = poly_reg.predict(x_data_poly) \n",
    "    \n",
    "    lineh.set_ydata(x_predicted)\n",
    "\n",
    "    \n",
    "d_slider.observe(update_plots_d ,'value')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-picture",
   "metadata": {},
   "outputs": [],
   "source": [
    "widgets.VBox([plots_d, d_slider],layout=layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southwest-strategy",
   "metadata": {},
   "source": [
    "<div  class=\"alert alert-block alert-info\"> \n",
    "   <ul>\n",
    "       <li>Un modelo con <strong>grado polinómico bajo</strong> tiene high bias, \n",
    "           por lo que el modelo predice pobremente el modelo \n",
    "           deseado.</li>\n",
    "       <li>Un modelo con <strong>grado polinómico alto</strong>, permite predecir \n",
    "            extremadamente bien los datos de entrenamiento, pero\n",
    "            no a los datos de prueba debido a que sufre de high\n",
    "            variance.</li>\n",
    "       <li>El <strong>grado de polinomio óptimo</strong> para utilizar en el algoritmo es aquel que \n",
    "           <strong>minimice</strong> la función de costo del set de datos de validación \n",
    "           cruzada $J_{cv}(\\Theta)$.</li>\n",
    "   </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peripheral-fisher",
   "metadata": {},
   "source": [
    "<h3>REGULARIZACIÓN</h3>\n",
    "\n",
    "Un valor de <strong>$\\lambda$ muy grande</strong> penaliza los parámetros de más, \n",
    "simplificando demasiado el modelo y causando <strong>underfitting</strong>.\n",
    "\n",
    "<ul>\n",
    "    <li>El ERROR DE ENTRENAMIENTO $J_{train}(\\Theta)$ tiende a <strong>crecer\n",
    "        </strong> al aumentar el parámetro de regularización.</li>\n",
    "    <li>El ERROR DE VALIDACIÓN CRUZADA $J_{cv}(\\Theta)$ tiende a decrecer\n",
    "        al aumentar el parámetro de regularización hasta un cierto punto, a \n",
    "        partir del cual comienza a aumentar, formando una <strong>curva convexa\n",
    "        </strong>.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-applicant",
   "metadata": {},
   "source": [
    "<strong>EJEMPLO:</strong> Variación de los errores de entrenamiento y validación cruzada con el parámetro de regularización:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-science",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    <li>Se define el grado del polinomio y se crea un modelo de regresión \n",
    "        polinómica. Para ello se utiliza <strong>PolynomialFeatures</strong> \n",
    "        de la librería <strong>scikit-learn</strong>.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conservative-combining",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_r = PolynomialFeatures(degree = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mechanical-mixture",
   "metadata": {},
   "source": [
    "<ol start=\"2\">\n",
    "    <li>Se transforman las variables tanto del set de entrenamiento como el de validación cruzada para que tengan el grado deseado.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-tiger",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_r = pf_r.fit_transform(x_train)\n",
    "x_cv_r = pf_r.fit_transform(x_cv)\n",
    "x_data_r = pf_r.fit_transform(x_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-miller",
   "metadata": {},
   "source": [
    "<ol start=\"3\">\n",
    "    <li>Se utiliza la clase <strong>Ridge</strong> para crear un modelo de \n",
    "        regresión con regulatización de tipo Ridge, el cual es entrenado \n",
    "        con el set de datos ya transformado al grado correspondiente.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-campbell",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "rid_reg = Ridge(alpha = 0, normalize = True)\n",
    "rid_reg.fit(x_train_r, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-posting",
   "metadata": {},
   "outputs": [],
   "source": [
    "rid_reg.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ambient-robert",
   "metadata": {},
   "source": [
    "<ol start=\"4\">\n",
    "    <li>Se calcula el costo tanto para el set de entrenamiento como el de \n",
    "        validación cruzada. En estos cálculos se utiliza la función costo\n",
    "        <strong>sin regularización</strong>.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrative-permission",
   "metadata": {},
   "outputs": [],
   "source": [
    "Jtrain_r = np.array([costFunction(rid_reg.predict(x_train_r), y_train)])\n",
    "Jcv_r = np.array([costFunction(rid_reg.predict(x_cv_r), y_cv)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "injured-musician",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_r = widgets.Output()\n",
    "\n",
    "with plots_r:\n",
    "    figs_r, axs_r = plt.subplots(2,1,figsize = (8,8),tight_layout = True)\n",
    "    figs_r.suptitle(r'INFLUENCIA DE LA REGULARIZACIÓN')\n",
    "    \n",
    "    axs_r[0].set_title(fr'Hipótesis vs Valores de salida reales')\n",
    "    axs_r[0].scatter(x_data, y_data, marker= 'o', s = 30, color ='black', alpha = 0.3, label = 'Datos')\n",
    "    lineh_r, = axs_r[0].plot(x_data,rid_reg.predict(x_data_r),label ='Valores predichos')\n",
    "    axs_r[0].set_xlabel('x')\n",
    "    axs_r[0].set_ylabel('y')\n",
    "    axs_r[0].grid(True)\n",
    "    axs_r[0].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "related-maximum",
   "metadata": {},
   "source": [
    "<ol start=\"5\">\n",
    "    <li>Se puede utilizar una función que cree el modelo de regresión con\n",
    "        la regularización deseada y lo entrene.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-light",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_reg(l, x_train):\n",
    "    rid_reg = Ridge(alpha = l, normalize = True)\n",
    "    rid_reg.fit(x_train, y_train)\n",
    "    \n",
    "    return rid_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriental-blackjack",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = np.arange(0.01,0.51,0.01)\n",
    "for l in L:\n",
    "    rid_reg = fit_reg(l, x_train_r)\n",
    "    \n",
    "    Jtrain_r = np.append(Jtrain_r,costFunction(rid_reg.predict(x_train_r), y_train)) \n",
    "    Jcv_r = np.append(Jcv_r,costFunction(rid_reg.predict(x_cv_r), y_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-stand",
   "metadata": {},
   "outputs": [],
   "source": [
    "axs_r[1].set_title(fr'Variación de función costo $J(\\theta)$')\n",
    "l = np.append(0,L)\n",
    "axs_r[1].plot(l, Jtrain_r, label = r'$J_{train}(\\Theta)$')\n",
    "axs_r[1].plot(l, Jcv_r, label = r'$J_{cv}(\\Theta)$')\n",
    "axs_r[1].set_xticks(l[::5])\n",
    "axs_r[1].set_xlabel(r'$\\lambda$')\n",
    "axs_r[1].set_ylabel(r'$J(\\Theta)$')\n",
    "axs_r[1].grid(True)\n",
    "axs_r[1].legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trained-exhibition",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_slider = widgets.FloatSlider(min = 0.0, max = 0.5, value = 0.0,step = 0.01, description = r'$\\lambda$')\n",
    "\n",
    "indx = math.ceil(l_slider.value*100)\n",
    "\n",
    "out_J_lambda = widgets.HTMLMath(value = fr'El valor de $Jcv(\\theta)$ es {Jcv_r[indx]:.3f}')\n",
    "    \n",
    "    \n",
    "def update_plots_r (change):\n",
    "    rid_reg = Ridge(alpha = l_slider.value, normalize = True)\n",
    "    rid_reg.fit(x_train_r, y_train)\n",
    "    x_predicted = rid_reg.predict(x_data_r),\n",
    "    lineh_r.set_ydata(x_predicted)\n",
    "    indx = math.ceil(l_slider.value*100)\n",
    "    out_J_lambda.value = fr'El valor de $Jcv(\\theta)$ es {Jcv_r[indx]:.3f}'\n",
    "\n",
    "    \n",
    "l_slider.observe(update_plots_r ,'value')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollow-horror",
   "metadata": {},
   "outputs": [],
   "source": [
    "widgets.VBox([plots_r, l_slider, out_J_lambda],layout = layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voluntary-voltage",
   "metadata": {},
   "source": [
    "<div  class=\"alert alert-block alert-info\"> \n",
    "   <ul>\n",
    "        <li>$\\lambda$ MUY GRANDE: Causa underfitting (High Bias).</li>\n",
    "        <li>$\\lambda$ MUY PEQUEÑO: Causa overfitting (High Variance).</li>\n",
    "        <li>$\\lambda$ ÓPTIMO: Minimiza el valor de $J_{cv}(\\Theta)$.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supported-samuel",
   "metadata": {},
   "source": [
    "<h3>SET DE ENTRENAMIENTO</h3>\n",
    "<dl> \n",
    "    <dt>HIGH BIAS (Underfitting):</dt>\n",
    "        <dd>\n",
    "            <ul>\n",
    "                <li>Set de entrenamiento <strong>pequeño</strong>:\n",
    "                    el valor de $J_{train}(\\Theta)$ es pequeño, \n",
    "                    mientras que el valor de $J_{cv}(\\Theta)$ es grande.\n",
    "                </li>\n",
    "                <li>Set de entrenamiento <strong>grande</strong>:\n",
    "                    tanto el valor de $J_{train}(\\Theta)$ como el de \n",
    "                    $J_{cv}(\\Theta)$ son grandes.\n",
    "                </li>\n",
    "            </ul>\n",
    "        </dd>\n",
    "</dl>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "living-header",
   "metadata": {},
   "source": [
    "<dl>\n",
    "    <dt>HIGH VARIANCE (Overfitting):</dt>\n",
    "        <dd>\n",
    "           <ul>\n",
    "                <li>Set de entrenamiento <strong>pequeño</strong>:\n",
    "                    el valor de $J_{train}(\\Theta)$ es pequeño, \n",
    "                    mientras que el valor de $J_{cv}(\\Theta)$ es grande.\n",
    "                </li>\n",
    "                <li>Set de entrenamiento <strong>grande</strong>:\n",
    "                    el valor de $J_{train}(\\Theta)$ se incrementa con el tamaño\n",
    "                    del set de entrenamiento, mientras que el valor de \n",
    "                    $J_{cv}(\\Theta)$ decrece. La diferencia entre ambos \n",
    "                    permanece significativa  $J_{cv}(\\Theta) > J_{train}(\\Theta)$.\n",
    "                </li>\n",
    "            </ul>\n",
    "        </dd>\n",
    "</dl>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entitled-winning",
   "metadata": {},
   "source": [
    "<strong>EJEMPLO:</strong> Variación de los errores de entrenamiento y validación cruzada con el tamaño del set de entrenamiento:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-netherlands",
   "metadata": {},
   "source": [
    "<div  class=\"alert alert-block alert-warning\"> \n",
    "   Para este ejemplo se dividió aleatoriamente el set de datos inicial en un \n",
    "   20% para validación cruzada y un 80% para entrenamiento. Luego se varió el \n",
    "   porcentaje de datos de entrenamiento utilizados de este 80% previamente \n",
    "   seleccionado. Por lo que <strong>todos los casos</strong> poseen el <strong>\n",
    "   mismo set de validación cruzada</strong>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documented-keeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridReg = Ridge(alpha = 0.5, normalize = True) #Bias\n",
    "\n",
    "pf_20 = PolynomialFeatures(degree = 20)\n",
    "\n",
    "x_data_20 = pf_20.fit_transform(x_data)\n",
    "x_cv_20 = pf_20.fit_transform(x_cv)\n",
    "\n",
    "def Jfunctions(ridReg):\n",
    "    M = np.arange(0.05, 1.0, 0.05)\n",
    "\n",
    "    Jtrain = np.array([])\n",
    "    Jcv = np.array([])\n",
    "\n",
    "    for m in M:\n",
    "\n",
    "        x_train_m, x_notUsed, y_train_m, y_notUsed = train_test_split(x_train, \\\n",
    "                                                    y_train, train_size = m, \\\n",
    "                                                    random_state = 1)\n",
    "\n",
    "        x_train_20 = pf_20.fit_transform(x_train_m)\n",
    "\n",
    "        ridReg.fit(x_train_20, y_train_m)\n",
    "\n",
    "        Jtrain = np.append(Jtrain, costFunction(ridReg.predict(x_train_20), \\\n",
    "                                                y_train_m)) \n",
    "        Jcv = np.append(Jcv, costFunction(ridReg.predict(x_cv_20), y_cv))\n",
    "\n",
    "    return Jtrain, Jcv\n",
    "\n",
    "Jtrain_values, Jcv_values = Jfunctions(ridReg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spiritual-hebrew",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makePredictions(ridReg, m):\n",
    "    x_train_m, x_notUsed, y_train_m, y_notUsed = train_test_split(x_train, \\\n",
    "                                                y_train, train_size = m, \\\n",
    "                                                random_state = 1)\n",
    "\n",
    "    x_train_20 = pf_20.fit_transform(x_train_m)\n",
    "\n",
    "    ridReg.fit(x_train_20, y_train_m)\n",
    "    \n",
    "    h = ridReg.predict(x_data_20)\n",
    "    \n",
    "    return h\n",
    "\n",
    "predictedValues = makePredictions(ridReg, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surface-allen",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_high = widgets.Output()\n",
    "\n",
    "with plots_high:\n",
    "    figs_high, axs_high = plt.subplots(2, 1, figsize = (8,8), tight_layout = True)\n",
    "    figs_high.suptitle(r'HIGH BIAS')\n",
    "    \n",
    "    axs_high[0].set_title(fr'Hipótesis vs Valores de salida reales')\n",
    "    axs_high[0].scatter(x_data, y_data, marker= 'o', s = 30, color ='black', alpha = 0.3, label = 'Datos')\n",
    "    lineh_high, = axs_high[0].plot(x_data, predictedValues,label ='Valores predichos')\n",
    "    axs_high[0].set_xlabel('x')\n",
    "    axs_high[0].set_ylabel('y')\n",
    "    axs_high[0].grid(True)\n",
    "    axs_high[0].legend()\n",
    "    \n",
    "    axs_high[1].set_title(fr'Variación de función costo $J(\\theta)$')\n",
    "    M = np.arange(0.05,1.0,0.05)\n",
    "    line_Jtrain, = axs_high[1].plot(M, Jtrain_values, label = r'$J_{train}(\\Theta)$')\n",
    "    line_Jcv, = axs_high[1].plot(M, Jcv_values, label = r'$J_{cv}(\\Theta)$')\n",
    "    axs_high[1].set_ylim(0,100)\n",
    "    axs_high[1].set_xlabel(r'm (tamaño set de entrenamiento)')\n",
    "    axs_high[1].set_ylabel(r'$J(\\Theta)$')\n",
    "    axs_high[1].grid(True)\n",
    "    axs_high[1].legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-reality",
   "metadata": {},
   "outputs": [],
   "source": [
    "M_slider = widgets.FloatSlider(min = 0.1, max = 0.9, value = 0.9, step = 0.1, description = 'm')\n",
    "\n",
    "highType = widgets.Dropdown(\n",
    "    options = ['Bias', 'Variance'],\n",
    "    value = 'Bias',\n",
    "    description ='High:',\n",
    "    layout = {'width': 'max-content'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sealed-hormone",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_plots_high (change):\n",
    "    \n",
    "    if highType.value == 'Bias':\n",
    "        ridReg = Ridge(alpha = 0.5, normalize = True)\n",
    "    else:\n",
    "        ridReg = Ridge(alpha = 0.0, normalize = True)\n",
    "    \n",
    "    Jtrain_values, Jcv_values = Jfunctions(ridReg)\n",
    "    line_Jtrain.set_ydata(Jtrain_values)\n",
    "    line_Jcv.set_ydata(Jcv_values)\n",
    "    \n",
    "    predictedValues = makePredictions(ridReg, M_slider.value)\n",
    "    \n",
    "    lineh_high.set_ydata(predictedValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial-congo",
   "metadata": {},
   "outputs": [],
   "source": [
    "M_slider.observe(update_plots_high,'value')\n",
    "highType.observe(update_plots_high,'value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "congressional-transsexual",
   "metadata": {},
   "outputs": [],
   "source": [
    "widgets.VBox([plots_high, M_slider, highType],layout=layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-integer",
   "metadata": {},
   "source": [
    "<div  class=\"alert alert-block alert-info\"> \n",
    "   <ul>\n",
    "        <li>Si un algoritmo sufre de high bias, aumentar la cantidad \n",
    "            de datos de entrenamiento no producirá una mejora.</li>\n",
    "        <li>Si un algoritmo sufre de high variance, aumentar la cantidad \n",
    "            de datos de entrenamiento generalmente produce mejoras.</li>\n",
    "       <li>El error obtenido en un algoritmo que sufre de high variance \n",
    "           por lo general es menor que el de un algoritmo que sufre de\n",
    "           high bias.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "employed-organizer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
