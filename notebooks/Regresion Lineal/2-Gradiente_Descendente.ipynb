{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "religious-issue",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "%matplotlib widget\n",
    "layout = widgets.Layout(align_items = 'center')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rapid-appointment",
   "metadata": {},
   "source": [
    "<h2>GRADIENTE DESCENDENTE</h2>\n",
    " \n",
    " Método de <strong>minimización de la función costo $J(\\theta)$</strong>.\n",
    "\n",
    "<ul>\n",
    "    <li>Graficar la función costo ($J$) como una función de \n",
    "        los parámetros $\\theta$ de la función hipótesis.</li>\n",
    "    <li>Hallar los valores mínimos en el gráfico calculando \n",
    "        la <strong>derivada</strong> de la función costo.</li>\n",
    "  <li> El valor de la derivada en un punto de la función es la \n",
    "      <strong>pendiente de la tangente</strong> a dicho punto. \n",
    "      Por lo tanto, el valor de la pendiente dará la \n",
    "      <strong>dirección</strong> hacia la cual es necesario cambiar \n",
    "      el valor de $\\theta$ para hallar el valor mínimo de la función \n",
    "      de costo $J(\\theta)$.</li>\n",
    "  <li> El tamaño de los pasos realizados son determinados por el \n",
    "      parámetro $\\alpha$ denominado <strong>tasa de aprendizaje</strong>. \n",
    "      Un $\\alpha$ pequeño resulta en un pequeño cambio del valor de \n",
    "      $\\theta$ y un $\\alpha$ grande, en un cambio mayor.</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "El algorítmo de gradiente descendente se basa en <strong>repetir hasta \n",
    "    la convergencia</strong>:\n",
    "\n",
    "$$\n",
    "    \\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial\\theta_j}J(\\theta_0, \\theta_1)\n",
    "$$\n",
    "\n",
    "<br>\n",
    "Si se considera que se tiene un único parámetro ($\\theta_1 = \\omega$), \n",
    "el algorítmo sería:\n",
    "Repetir hasta la convergencia: $\\theta_1 := \\theta_1 - \\alpha \\frac{\\partial}{\\partial\\theta_1}J(\\theta_1)$\n",
    "\n",
    "<center>\n",
    "<img src=\"./figures/gradientDesc.jpeg\" height=\"600\" width=\"600\"/>\n",
    "</center>\n",
    "\n",
    "- Cuando la pendiente es negativa, el valor de $\\theta_1$ es incrementado \n",
    "($\\theta_1 := \\theta_1 - \\alpha.\\text{valor negativo}$), desplazandonos \n",
    "hacia la derecha en el gráfico de $J(\\omega)$\n",
    "- Cuando la pendiente es positiva, el valor de $\\theta_1$ decrece \n",
    "($\\theta_1 := \\theta_1 - \\alpha.\\text{valor positivo}$) desplazandonos hacia \n",
    "la izquierda en el gráfico de $J(\\omega)$\n",
    "- En el mínimo de la función,$\\frac{\\partial}{\\partial\\theta_1}J(\\theta_1)$ \n",
    "será siempre igual a cero.\n",
    " \n",
    "<b>Cosas a tener en cuenta:</b>\n",
    "<ol>\n",
    "    <li>Si $\\alpha$ es muy pequeño, el gradiente descendente puede ser muy \n",
    "        lento.</li>\n",
    "    <li>Si $\\alpha$ es muy grande, el gradiente descendente puede no converger \n",
    "        nunca.</li>\n",
    "    <li>Dependiendo del punto de partida, se podrían obtener distintos valores, \n",
    "        correspondientes a distintos valores mínimos. Esto no sucede en la \n",
    "        regresión lineal ya que la función $J(\\theta)$ tiene un único mínimo \n",
    "        global.</li>\n",
    "    <li>La función de costo $J(\\theta)$ debería decrecer su valor en cada iteración \n",
    "        si el gradiente descendente funciona correctamente.</li>\n",
    "</ol>\n",
    "\n",
    "<center>\n",
    "<img src=\"./figures/learning-rate.png\" height=\"600\" width=\"600\"/>\n",
    "</center>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "antique-roberts",
   "metadata": {},
   "source": [
    "<strong>EJEMPLO CON REGRESIÓN LINEAL</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessory-avatar",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_linearR_gd = np.linspace(0, 1, 10)\n",
    "y_linearR_gd = x_linearR_gd\n",
    "\n",
    "alpha_slider = widgets.FloatSlider(min = 0,max = 0.99,step = 0.001,value = 0.5,description = r'$\\alpha$')\n",
    "\n",
    "def plot_J_gd(alpha, x, y):\n",
    "    m = x.size\n",
    "    \n",
    "    t0 = np.array([0])\n",
    "    t1 = np.array([0])\n",
    "    J = np.array([])\n",
    "\n",
    "    for i in range(0,50):\n",
    "        h = t0[i] + t1[i] * x\n",
    "        J = np.append(J, 1/(2 * m) * np.sum(np.square(h - y)))\n",
    "\n",
    "        t0 = np.append(t0, t0[i] - alpha/m * np.sum(h - y))\n",
    "        t1 = np.append(t1, t1[i] - alpha/m * np.sum((h - y) * x))\n",
    "        \n",
    "    return t0[-1], t1[-1], J\n",
    "\n",
    "def h_gd(t0,t1,x):\n",
    "    h = t0 + t1 * x\n",
    "    return h\n",
    "    \n",
    "plots_lr_gd = widgets.Output()\n",
    "\n",
    "with plots_lr_gd:\n",
    "    fig_lr_gd, axs_lr_gd = plt.subplots(1,2,figsize = (10,4),tight_layout = True)\n",
    "    fig_lr_gd.suptitle(r'Regresión lineal con gradiente descendente')\n",
    "    \n",
    "    axs_lr_gd[1].set_title(fr'Variación de función de costo $J(\\theta)$')\n",
    "    t0, t1, J_linearR_gd = plot_J_gd(alpha_slider.value, x_linearR_gd, y_linearR_gd)\n",
    "    line3, = axs_lr_gd[1].plot(J_linearR_gd)\n",
    "    axs_lr_gd[1].set_ylabel(r'$J(\\theta)$')   \n",
    "    axs_lr_gd[1].set_xlabel(r'N° de iteraciones')   \n",
    "    axs_lr_gd[1].grid(True)\n",
    "    \n",
    "    axs_lr_gd[0].set_title(fr'Hipótesis vs Valores de salida reales')\n",
    "    line4, = axs_lr_gd[0].plot(x_linearR_gd, h_gd(t0, t1, x_linearR_gd),label='Valor predicho')\n",
    "    axs_lr_gd[0].plot(x_linearR_gd, y_linearR_gd,'o', label = 'Salida real')\n",
    "    axs_lr_gd[0].legend()\n",
    "    axs_lr_gd[0].grid(True)\n",
    "    \n",
    "\n",
    "out_t0_linearR_gd = widgets.HTMLMath(value = fr'El valor de $\\theta_0$ después de 50 iteraciones es: {t0:.2f}')\n",
    "    \n",
    "out_t1_linearR_gd = widgets.HTMLMath(value = fr'El valor de $\\theta_1$ después de 50 iteraciones es: {t1:.2f}')\n",
    "\n",
    "out_linearR_gd = widgets.HTMLMath(value = fr'Los valores correctos serían $\\theta_0=0$ y $\\theta_1=1$')\n",
    "\n",
    "def update_J_linearR_gd (change):\n",
    "    t0, t1, J_linearR_gd = plot_J_gd(alpha_slider.value, x_linearR_gd, y_linearR_gd)\n",
    "    line3.set_ydata(J_linearR_gd)\n",
    "    line4.set_ydata(h_gd(t0, t1,x_linearR_gd))\n",
    "    out_t0_linearR_gd.value = fr'El valor de $\\theta_0$ después de 50 iteraciones es: {t0:.2f}'\n",
    "    out_t1_linearR_gd.value = fr'El valor de $\\theta_1$ después de 50 iteraciones es: {t1:.2f}'\n",
    "    \n",
    "alpha_slider.observe(update_J_linearR_gd ,'value')\n",
    "\n",
    "widgets.VBox([plots_lr_gd,alpha_slider, out_t0_linearR_gd, out_t1_linearR_gd,out_linearR_gd],layout=layout)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-wound",
   "metadata": {},
   "source": [
    "<h3>TRATAMIENTO DE VARIABLES</h3>\n",
    "Para mejorar el desempeño del gradiente descendente, los \n",
    "<strong>valores de entrada</strong> deben ubicarse en el \n",
    "<strong>mismo rango</strong>:\n",
    "$$\n",
    "    -1 \\leq X_j \\leq 1\n",
    "$$\n",
    "$$\n",
    "    -0.5 \\leq X_j \\leq 0.5\n",
    "$$\n",
    "<dl>\n",
    "  <dt>Escalamiento de variables:</dt>\n",
    "  <dd>- Dividir los valores de entrada por el rango (valor\n",
    "      máximo menos valor mínimo) de la misma variable, \n",
    "      resultando en un nuevo rango de 1. Es posible utilizar \n",
    "      también la desviación estandar de la variable para realizar \n",
    "      su escalamiento.</dd>\n",
    "<br>\n",
    "  <dt>Normalización de media:</dt>\n",
    "  <dd>- Restar el valor promedio de una variable a sus valores \n",
    "      de entrada, resultando en un nuevo valor promedio para esa \n",
    "      variable de 0.</dd>\n",
    "</dl>\n",
    "    $$\n",
    "        x_i := \\frac{x_i-\\mu_i}{s_i}\n",
    "    $$\n",
    "\n",
    "<ul>\n",
    "    <li>$\\mu_i$: PROMEDIOS de todos los valores de la variable $i$.</li>\n",
    "    <li>$s_i$: RANGO de valores de la variable $i$ o DESVIACIÓN \n",
    "        ESTANDAR.</li>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
